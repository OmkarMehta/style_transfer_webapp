{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "id": "yhott99a81l1",
        "outputId": "ebf71934-cdf3-4df6-9751-f72e530df062"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "Collecting Pillow\n",
            "  Downloading Pillow-9.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 14.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: Pillow\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed Pillow-9.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-05 16:02:37--  http://images.cocodataset.org/zips/val2014.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.46.108\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.46.108|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6645013297 (6.2G) [application/zip]\n",
            "Saving to: ‘val2014.zip’\n",
            "\n",
            "val2014.zip         100%[===================>]   6.19G  32.9MB/s    in 3m 19s  \n",
            "\n",
            "2022-05-05 16:05:56 (31.8 MB/s) - ‘val2014.zip’ saved [6645013297/6645013297]\n",
            "\n",
            "/content/train\n",
            "/content\n",
            "--2022-05-05 16:06:36--  https://web.eecs.umich.edu/~justincj/models/vgg16-00b39a1b.pth\n",
            "Resolving web.eecs.umich.edu (web.eecs.umich.edu)... 141.212.113.214\n",
            "Connecting to web.eecs.umich.edu (web.eecs.umich.edu)|141.212.113.214|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 553451520 (528M) [application/x-tar]\n",
            "Saving to: ‘vgg16-00b39a1b.pth’\n",
            "\n",
            "vgg16-00b39a1b.pth  100%[===================>] 527.81M  29.5MB/s    in 18s     \n",
            "\n",
            "2022-05-05 16:06:56 (28.5 MB/s) - ‘vgg16-00b39a1b.pth’ saved [553451520/553451520]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# install pillow\n",
        "!pip install Pillow -U\n",
        "\n",
        "# download and unzip dataset o train\n",
        "!wget http://images.cocodataset.org/zips/val2014.zip\n",
        "!mkdir train\n",
        "%cd /content/train\n",
        "!unzip -qq /content/val2014.zip\n",
        "%cd ..\n",
        "\n",
        "#download model\n",
        "!wget https://web.eecs.umich.edu/~justincj/models/vgg16-00b39a1b.pth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --ignore-installed Pillow==9.0.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "tSYd5NZ64FxC",
        "outputId": "89bd3b73-70c1-4fb4-c431-52ca11ece7ff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Pillow==9.0.0\n",
            "  Downloading Pillow-9.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 15.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: Pillow\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed Pillow-9.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UJNuZHHC81l1"
      },
      "outputs": [],
      "source": [
        "import PIL\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import vgg16_fe as vgg # this is used to extract features\n",
        "import vgg16 as vgg_model # this is used to train the model\n",
        "import utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tqcq7gT_tbl",
        "outputId": "6e6e94e7-8073-4241-e2a7-0c6caf9a223c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcrSxMYUhN-7"
      },
      "source": [
        "# Trained on COCO dataset for VGG16 Model for different styles\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBCasn9wh141"
      },
      "source": [
        "## Rain Princess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VVYnNUQ881l2"
      },
      "outputs": [],
      "source": [
        "# Global variables\n",
        "TRAIN_IMAGE_SIZE = 256\n",
        "DATASET_PATH = \"train\"\n",
        "NUM_EPOCHS = 1\n",
        "STYLE_IMAGE_PATH = \"style/rain_princess.jpg\"\n",
        "BATCH_SIZE = 4 \n",
        "CONTENT_WEIGHT = 30 # 17\n",
        "STYLE_WEIGHT = 50 # 25\n",
        "ADAM_LR = 0.001\n",
        "SAVE_MODEL_PATH = \"rain_princess/models/\"\n",
        "SAVE_IMAGE_PATH = \"rain_princess/images/\"\n",
        "SAVE_MODEL_EVERY = 500 # 2,000 Images with batch size 4\n",
        "SEED = 35\n",
        "PLOT_LOSS = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "opY3yaMs81l2"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    # seeds\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    random.seed(SEED)\n",
        "\n",
        "    # get the device\n",
        "    device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # load the training dataset\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(TRAIN_IMAGE_SIZE),\n",
        "        transforms.CenterCrop(TRAIN_IMAGE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.mul(255))\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.ImageFolder(DATASET_PATH, transform=transform)\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    # load the models and start training :)\n",
        "    model = vgg_model.VGG16('/content/vgg16-00b39a1b.pth').to(device) #Creating a pretrained model object by loading the downloaded .pth file. This model object will be retrained \n",
        "    vgg16_fe = vgg.VGG16('/content/vgg16-00b39a1b.pth').to(device) #Creating a pretrained model object by loading the downloaded .pth file. This model object will be used to extract features and will not be retrained\n",
        "\n",
        "    # get style features\n",
        "    imagenet_neg_mean = torch.tensor([-103.939, -116.779, -123.68], dtype=torch.float32).reshape(1,3,1,1).to(device)\n",
        "    # load the style image\n",
        "    style_image = utils.load_image(STYLE_IMAGE_PATH)\n",
        "    # convert style image to tensor\n",
        "    style_tensor = utils.itot(style_image).to(device)\n",
        "    # add imagenet mean\n",
        "    style_tensor = style_tensor.add(imagenet_neg_mean)\n",
        "\n",
        "    B, C, H, W = style_tensor.shape\n",
        "    # get the features of the style image\n",
        "    style_features = vgg16_fe(style_tensor.expand([BATCH_SIZE, C, H, W]))\n",
        "\n",
        "    # get the gram matrix of the style image\n",
        "    style_gram = {}\n",
        "    for key, value in style_features.items():\n",
        "        style_gram[key] = utils.gram(value)\n",
        "\n",
        "    # Optimizer settings\n",
        "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=ADAM_LR)\n",
        "\n",
        "    # Loss trackers\n",
        "    content_loss_history = []\n",
        "    style_loss_history = []\n",
        "    total_loss_history = []\n",
        "    batch_content_loss_sum = 0\n",
        "    batch_style_loss_sum = 0\n",
        "    batch_total_loss_sum = 0\n",
        "\n",
        "    # Optimization/Training Loop\n",
        "    batch_count = 1\n",
        "    start_time = time.time()\n",
        "\n",
        "    # training loop\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        print(\"========Epoch {}/{}========\".format(epoch+1, NUM_EPOCHS))\n",
        "        for content_batch, _ in train_loader:\n",
        "            # Get current batch size in case of odd batch sizes\n",
        "            curr_batch_size = content_batch.shape[0]\n",
        "\n",
        "            # free up unnecessary memory of cuda\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            # zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Generate images and get features\n",
        "            content_batch = content_batch[:,[2,1,0]].to(device)\n",
        "            generated_batch = model(content_batch)\n",
        "\n",
        "            # print(\"Content Batch: \", content_batch.shape)\n",
        "            # print(\"Generated Batch: \", generated_batch.shape)\n",
        "\n",
        "            content_features = vgg16_fe(content_batch.add(imagenet_neg_mean))\n",
        "            generated_features = vgg16_fe(generated_batch.add(imagenet_neg_mean))\n",
        "\n",
        "            # print(\"Generated Features:\", generated_features['relu2_2'].shape)\n",
        "            # print(\"Content Features:\", content_features['relu2_2'].shape)\n",
        "\n",
        "            # Calculate content loss\n",
        "            MSELoss = nn.MSELoss().to(device)\n",
        "            content_loss = CONTENT_WEIGHT * MSELoss(generated_features['relu1_2'], content_features['relu1_2'])            \n",
        "            batch_content_loss_sum += content_loss\n",
        "\n",
        "            # Calculate style loss\n",
        "            style_loss = 0\n",
        "            for key, value in generated_features.items():\n",
        "                s_loss = MSELoss(utils.gram(value), style_gram[key][:curr_batch_size])\n",
        "                style_loss += s_loss\n",
        "            style_loss *= STYLE_WEIGHT\n",
        "            batch_style_loss_sum += style_loss.item()\n",
        "\n",
        "            # Calculate total loss\n",
        "            total_loss = content_loss + style_loss\n",
        "            batch_total_loss_sum += total_loss.item()\n",
        "\n",
        "            # Backpropagation\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Save Model and Print Losses\n",
        "            if (((batch_count-1)%SAVE_MODEL_EVERY == 0) or (batch_count==NUM_EPOCHS*len(train_loader))):\n",
        "                # Print Losses\n",
        "                print(\"========Iteration {}/{}========\".format(batch_count, NUM_EPOCHS*len(train_loader)))\n",
        "                print(\"\\tContent Loss:\\t{:.2f}\".format(batch_content_loss_sum/batch_count))\n",
        "                print(\"\\tStyle Loss:\\t{:.2f}\".format(batch_style_loss_sum/batch_count))\n",
        "                print(\"\\tTotal Loss:\\t{:.2f}\".format(batch_total_loss_sum/batch_count))\n",
        "                print(\"Time elapsed:\\t{} seconds\".format(time.time()-start_time))\n",
        "\n",
        "                # Save Model\n",
        "                checkpoint_path = SAVE_MODEL_PATH + \"checkpoint_\" + str(batch_count-1) + \".pth\"\n",
        "                torch.save(model.state_dict(), checkpoint_path)\n",
        "                \n",
        "                print(\"Saved self trained VGG16 checkpoint file at {}\".format(checkpoint_path))\n",
        "\n",
        "                # Save sample generated image\n",
        "                sample_tensor = generated_batch[0].clone().detach().unsqueeze(dim=0) # clone to save inplace errors\n",
        "                sample_image = utils.ttoi(sample_tensor.clone().detach())\n",
        "                sample_image_path = SAVE_IMAGE_PATH + \"sample0_\" + str(batch_count-1) + \".png\"\n",
        "                utils.saveimg(sample_image, sample_image_path)\n",
        "                utils.show(sample_image)\n",
        "                print(\"Saved sample stylized image at {}\".format(sample_image_path))\n",
        "\n",
        "                # Save loss histories\n",
        "                content_loss_history.append(batch_total_loss_sum/batch_count)\n",
        "                style_loss_history.append(batch_style_loss_sum/batch_count)\n",
        "                total_loss_history.append(batch_total_loss_sum/batch_count)\n",
        "            \n",
        "            # increment batch count\n",
        "            batch_count += 1\n",
        "    \n",
        "    stop_time = time.time()\n",
        "    # Print loss histories\n",
        "    print(\"Done Training the Network!\")\n",
        "    print(\"Training Time: {} seconds\".format(stop_time-start_time))\n",
        "    print(\"========Content Loss========\")\n",
        "    print(content_loss_history) \n",
        "    print(\"========Style Loss========\")\n",
        "    print(style_loss_history) \n",
        "    print(\"========Total Loss========\")\n",
        "    print(total_loss_history) \n",
        "\n",
        "    # Save TransformerNetwork weights\n",
        "    model.eval()\n",
        "    model.cpu()\n",
        "    final_path = SAVE_MODEL_PATH + \"transformer_weight.pth\"\n",
        "    print(\"Saving VGG16 weights at {}\".format(final_path))\n",
        "    torch.save(model.state_dict(), final_path)\n",
        "    print(\"Done saving final model\")\n",
        "\n",
        "    # Plot Loss Histories\n",
        "    if (PLOT_LOSS):\n",
        "        utils.plot_loss_hist(content_loss_history, style_loss_history, total_loss_history)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwQvKPz6kUK8"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "t--_u4EVAyCN",
        "outputId": "0d71095f-801a-4a43-d87a-913c34bbe44b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========Epoch 1/1========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([4, 64, 256, 256])) that is different to the input size (torch.Size([4, 64, 257, 257])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-0b7cf335845a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m# Calculate content loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mMSELoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mcontent_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCONTENT_WEIGHT\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'relu1_2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'relu1_2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0mbatch_content_loss_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcontent_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3259\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3261\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3262\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (257) must match the size of tensor b (256) at non-singleton dimension 3"
          ]
        }
      ],
      "source": [
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNHJ09keAlJM"
      },
      "outputs": [],
      "source": [
        "# copy the models folder to the folder in the google drive\n",
        "!cp -r rain_princess/models/ /content/gdrive/My\\ Drive/vgg_coco/rain_princess/\n",
        "# copy the images folder to the folder in the google drive\n",
        "!cp -r rain_princess/images/ /content/gdrive/My\\ Drive/vgg_coco/rain_princess/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2n91vZ8mB734"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfGzAPPHiDJR"
      },
      "source": [
        "## The Scream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzVv-u32iOA2"
      },
      "outputs": [],
      "source": [
        "# Global variables\n",
        "TRAIN_IMAGE_SIZE = 256\n",
        "DATASET_PATH = \"train\"\n",
        "NUM_EPOCHS = 1\n",
        "STYLE_IMAGE_PATH = \"style/the_scream.jpg\"\n",
        "BATCH_SIZE = 4 \n",
        "CONTENT_WEIGHT = 30 # 17\n",
        "STYLE_WEIGHT = 50 # 25\n",
        "ADAM_LR = 0.001\n",
        "SAVE_MODEL_PATH = \"the_scream/models/\"\n",
        "SAVE_IMAGE_PATH = \"the_scream/images/\"\n",
        "SAVE_MODEL_EVERY = 500 # 2,000 Images with batch size 4\n",
        "SEED = 35\n",
        "PLOT_LOSS = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crYKIib3ka2R"
      },
      "outputs": [],
      "source": [
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BM1BmtuOk6QI"
      },
      "outputs": [],
      "source": [
        "# copy the models folder to the folder in the google drive\n",
        "!cp -r the_scream/models/ /content/gdrive/My\\ Drive/vgg_coco/the_scream/\n",
        "# copy the images folder to the folder in the google drive\n",
        "!cp -r the_scream/images/ /content/gdrive/My\\ Drive/vgg_coco/the_scream/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tC7YwTHIiFwN"
      },
      "source": [
        "## The Shipwreck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29upo3CiiPHq"
      },
      "outputs": [],
      "source": [
        "# Global variables\n",
        "TRAIN_IMAGE_SIZE = 256\n",
        "DATASET_PATH = \"train\"\n",
        "NUM_EPOCHS = 1\n",
        "STYLE_IMAGE_PATH = \"style/the_shipwreck.jpg\"\n",
        "BATCH_SIZE = 4 \n",
        "CONTENT_WEIGHT = 30 # 17\n",
        "STYLE_WEIGHT = 50 # 25\n",
        "ADAM_LR = 0.001\n",
        "SAVE_MODEL_PATH = \"the_shipwreck/models/\"\n",
        "SAVE_IMAGE_PATH = \"the_shipwreck/images/\"\n",
        "SAVE_MODEL_EVERY = 500 # 2,000 Images with batch size 4\n",
        "SEED = 35\n",
        "PLOT_LOSS = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOhsbvaskd0o"
      },
      "outputs": [],
      "source": [
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyyB7AetlT4d"
      },
      "outputs": [],
      "source": [
        "# copy the models folder to the folder in the google drive\n",
        "!cp -r the_shipwreck/models/ /content/gdrive/My\\ Drive/vgg_coco/the_shipwreck/\n",
        "# copy the images folder to the folder in the google drive\n",
        "!cp -r the_shipwreck/images/ /content/gdrive/My\\ Drive/vgg_coco/the_shipwreck/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLBTCbNBiH3H"
      },
      "source": [
        "## Udnie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adsUU-J-iPmJ"
      },
      "outputs": [],
      "source": [
        "# Global variables\n",
        "TRAIN_IMAGE_SIZE = 256\n",
        "DATASET_PATH = \"train\"\n",
        "NUM_EPOCHS = 1\n",
        "STYLE_IMAGE_PATH = \"style/udnie.jpg\"\n",
        "BATCH_SIZE = 4 \n",
        "CONTENT_WEIGHT = 30 # 17\n",
        "STYLE_WEIGHT = 50 # 25\n",
        "ADAM_LR = 0.001\n",
        "SAVE_MODEL_PATH = \"udnie/models/\"\n",
        "SAVE_IMAGE_PATH = \"udnie/images/\"\n",
        "SAVE_MODEL_EVERY = 500 # 2,000 Images with batch size 4\n",
        "SEED = 35\n",
        "PLOT_LOSS = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvW2EAHbkgEH"
      },
      "outputs": [],
      "source": [
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2obmNWDlbD1"
      },
      "outputs": [],
      "source": [
        "# copy the models folder to the folder in the google drive\n",
        "!cp -r udnie/models/ /content/gdrive/My\\ Drive/vgg_coco/udnie/\n",
        "# copy the images folder to the folder in the google drive\n",
        "!cp -r udnie/images/ /content/gdrive/My\\ Drive/vgg_coco/udnie/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G36r0NG6iJOV"
      },
      "source": [
        "## Wave"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vymj0QMiQZ-"
      },
      "outputs": [],
      "source": [
        "# Global variables\n",
        "TRAIN_IMAGE_SIZE = 256\n",
        "DATASET_PATH = \"train\"\n",
        "NUM_EPOCHS = 1\n",
        "STYLE_IMAGE_PATH = \"style/wave.jpg\"\n",
        "BATCH_SIZE = 4 \n",
        "CONTENT_WEIGHT = 30 # 17\n",
        "STYLE_WEIGHT = 50 # 25\n",
        "ADAM_LR = 0.001\n",
        "SAVE_MODEL_PATH = \"wave/models/\"\n",
        "SAVE_IMAGE_PATH = \"wave/images/\"\n",
        "SAVE_MODEL_EVERY = 500 # 2,000 Images with batch size 4\n",
        "SEED = 35\n",
        "PLOT_LOSS = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TwuUY7OkiFB"
      },
      "outputs": [],
      "source": [
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAXOIJJZlfOB"
      },
      "outputs": [],
      "source": [
        "# copy the models folder to the folder in the google drive\n",
        "!cp -r wave/models/ /content/gdrive/My\\ Drive/vgg_coco/wave/\n",
        "# copy the images folder to the folder in the google drive\n",
        "!cp -r wave/images/ /content/gdrive/My\\ Drive/vgg_coco/wave/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKz3_HBmmw8K"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "vgg_coco.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}