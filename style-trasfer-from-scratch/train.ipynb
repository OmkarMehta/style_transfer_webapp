{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKe6deWiEPes"
      },
      "source": [
        "# Style Transfer Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HENPse6EPeu"
      },
      "source": [
        "## Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PS4y2GIfEPeu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as T \n",
        "import PIL\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "\n",
        "# get utils functions\n",
        "from utils import grammatrix, loss_of_style, loss_of_content, content_transform, detransform_content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W6eMm9tEPev"
      },
      "source": [
        "## Get data from images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ATovgDzBEPew"
      },
      "outputs": [],
      "source": [
        "# The content is the image of the user. The style is the image of the style.\n",
        "# content image is stored in images/omkar.jpeg and style image is stored in images/generosity.jpeg\n",
        "content_image = PIL.Image.open(\"images/omkar.jpg\")\n",
        "style_image = PIL.Image.open(\"images/generosity.jpeg\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAkxT3e6EPew"
      },
      "source": [
        "## Get pretrained models of vgg16 and resnet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GgU7T8yDEPew"
      },
      "outputs": [],
      "source": [
        "# Load VGG16 model\n",
        "# The VGG16 model is a pretrained CNN model that was trained on the ImageNet dataset.\n",
        "vgg16 = torchvision.models.vgg16(pretrained=True).features\n",
        "\n",
        "# Load ResNet18 model\n",
        "resnet18 = torchvision.models.resnet18(pretrained=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16[21]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCPUPBaJIRUF",
        "outputId": "2ce36351-db9e-4dec-8839-a01ce85a78f6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnet18.layer2[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fquBvoF4HqnF",
        "outputId": "2e5990e4-433c-42f9-cb03-15c91fe82f64"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BasicBlock(\n",
              "  (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TK3foer_HoqH",
        "outputId": "03d04b32-c863-4966-8f61-eae6252cc59c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (1): ReLU(inplace=True)\n",
              "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (3): ReLU(inplace=True)\n",
              "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (6): ReLU(inplace=True)\n",
              "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (8): ReLU(inplace=True)\n",
              "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (11): ReLU(inplace=True)\n",
              "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (13): ReLU(inplace=True)\n",
              "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (15): ReLU(inplace=True)\n",
              "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (18): ReLU(inplace=True)\n",
              "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (20): ReLU(inplace=True)\n",
              "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (22): ReLU(inplace=True)\n",
              "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (25): ReLU(inplace=True)\n",
              "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (27): ReLU(inplace=True)\n",
              "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (29): ReLU(inplace=True)\n",
              "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9ORjfX8Gc0g",
        "outputId": "a4b7a584-0323-401f-b977-92e016b5cf53"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQUnGYlhEPex"
      },
      "source": [
        "## train function for style transfer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QT7ecjoTEPey"
      },
      "outputs": [],
      "source": [
        "# defining get_features function \n",
        "def get_features(image, model, model_name):\n",
        "    '''\n",
        "    Getting feature map from each layer of the model\n",
        "    '''\n",
        "    features = list()\n",
        "    first_feature = image\n",
        "    if model_name == 'vgg16':\n",
        "        model = model.features\n",
        "    # we add image as the input to the model\n",
        "    for idx, layer in enumerate(model._modules.values()):\n",
        "        image = layer(first_feature)\n",
        "        features.append(image)\n",
        "        first_feature = image\n",
        "    return features "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zJ5ZrS_QEPey"
      },
      "outputs": [],
      "source": [
        "def train_st(style, style_layer, style_weight, content, content_layer, content_weight, style_size, image_size, device, model, style_image_name):\n",
        "    '''\n",
        "    Train Style Transfer, based on Gatys' paper.\n",
        "    :Input:\n",
        "    - style: PyTorch tensor, style image\n",
        "    - style_layer: int, index of style layer\n",
        "    - style_weight: float, weight of style layer\n",
        "    - content: PyTorch tensor, content image\n",
        "    - content_layer: int, index of content layer\n",
        "    - content_weight: float, weight of content layer\n",
        "    - style_size: int, size of style image\n",
        "    - image_size: int, size of image\n",
        "    - device: PyTorch device\n",
        "    - model: 'vgg16' or 'resnet18'\n",
        "    - style_image_name: string, name of style image\n",
        "    \n",
        "    :Output:\n",
        "    - image: PyTorch tensor, image\n",
        "    '''\n",
        "    # model \n",
        "    if model == 'vgg16':\n",
        "        model_1 = torchvision.models.vgg16(pretrained=True).to(device)\n",
        "    elif model == 'resnet18':\n",
        "        model_1 = torchvision.models.resnet18(pretrained=True).to(device)\n",
        "\n",
        "    # get style transform\n",
        "    style = content_transform(style, image_size)\n",
        "    # convert type of style to float\n",
        "    style = style.type(torch.cuda.FloatTensor).to(device)\n",
        "\n",
        "    # get content transform\n",
        "    content = content_transform(content, image_size)\n",
        "    # convert type to float\n",
        "    content = content.type(torch.cuda.FloatTensor).to(device)\n",
        "\n",
        "    # get style and content features\n",
        "    content_features = get_features(content, model_1, model)\n",
        "    style_features = get_features(style, model_1, model)\n",
        "\n",
        "    # the layer on which we want the content to be \n",
        "    content_layer_output = content_features[content_layer]\n",
        "    # the layers on which we want the style to be\n",
        "    style_layers_output = [grammatrix(style_features[layer]) for layer in style_layer]\n",
        "\n",
        "    # First, initialize the image on which we want to perform style transfer with type torch.FloatTensor\n",
        "    image = content.clone().type(torch.cuda.FloatTensor).requires_grad_(True).to(device)\n",
        "    \n",
        "    # Hyperparameters\n",
        "    num_epochs = 25\n",
        "    learning_rate = 1e-3\n",
        "\n",
        "    # define the optimizer as Adam\n",
        "    optimizer = torch.optim.Adam([image], lr=learning_rate)\n",
        "\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # model train\n",
        "        model_1.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # get the features of the image\n",
        "        image_features = get_features(image, model_1, model)\n",
        "\n",
        "        # get the content loss\n",
        "        content_loss = loss_of_content(image_features[content_layer], content_weight, content_layer_output)\n",
        "\n",
        "        # get the style loss\n",
        "        style_loss = loss_of_style(image_features, style_layer, style_layers_output, style_weight)\n",
        "\n",
        "        # get the total loss\n",
        "        total_loss = content_loss + style_loss\n",
        "\n",
        "        # perform backpropagation\n",
        "        total_loss.backward()\n",
        "\n",
        "        # update the image\n",
        "        optimizer.step()\n",
        "\n",
        "        \n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(\"Epoch {}:\".format(epoch))\n",
        "            print(\"Content Loss: {:.4f}\".format(content_loss.item()))\n",
        "            print(\"Style Loss: {:.4f}\".format(style_loss.item()))\n",
        "            print(\"Total Loss: {:.4f}\".format(total_loss.item()))\n",
        "            print()\n",
        "\n",
        "            # display the image\n",
        "            plt.axis(\"off\")\n",
        "            plt.imshow(detransform_content(image.data.cpu(), image_size))\n",
        "            plt.show()\n",
        "\n",
        "            # save the checkpoint\n",
        "            # if checkpoints folder doesn't exist, create it\n",
        "            if not os.path.exists(\"checkpoints\"):\n",
        "                os.mkdir(\"checkpoints\")\n",
        "            # save the checkpoint of the model\n",
        "            if total_loss.item() < best_loss:\n",
        "                model_1.eval()\n",
        "                best_loss = total_loss.item()\n",
        "                torch.save(model_1.state_dict(), \"checkpoints/{}_model_{}.pth\".format(style_image_name, epoch))\n",
        "                # save the image in pth format\n",
        "                torch.save(image.data, \"checkpoints/{}_image_{}.pth\".format(style_image_name, epoch))\n",
        "                print(\"Saved checkpoint of model and image\")\n",
        "        \n",
        "        if best_loss > total_loss.item():\n",
        "            # save the best model and image\n",
        "            model_1.eval()\n",
        "            best_loss = total_loss.item()\n",
        "            torch.save(model_1.state_dict(), \"checkpoints/{}_model_best.pth\".format(style_image_name))\n",
        "            torch.save(image.data, \"checkpoints/{}_image_best.pth\".format(style_image_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "JebENVZYG7E3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters for train_st for vgg\n",
        "\n",
        "parameters = dict(\n",
        "    style = style_image,\n",
        "    # get any layers of vgg model\n",
        "    style_layer = (21, 24, 26, 28),\n",
        "    style_weight = (3e3, 3e2, 3e1, 3e0),\n",
        "    content = content_image,\n",
        "    content_layer = 15,\n",
        "    content_weight = 3e-2,\n",
        "    style_size = 512,\n",
        "    image_size = 512,\n",
        "    device = device,\n",
        "    model = 'vgg16',\n",
        "    style_image_name = 'generosity'\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "kCAibtCmFjj2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_st(**parameters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "ybUev3dpI_rG",
        "outputId": "3eef0133-c43b-4cc4-943a-20e40cf58cb2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0:\n",
            "Content Loss: 0.0000\n",
            "Style Loss: 319227537915904.0000\n",
            "Total Loss: 319227537915904.0000\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-2fa4c3f36e3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_st\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-0387f02ec928>\u001b[0m in \u001b[0;36mtrain_st\u001b[0;34m(style, style_layer, style_weight, content, content_layer, content_weight, style_size, image_size, device, model, style_image_name)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# display the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetransform_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: detransform_content() missing 1 required positional argument: 'image_size'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADnCAYAAAC9roUQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAADKUlEQVR4nO3UMQEAIAzAMMC/5+GiHCQKenXPzAKgcV4HAPzEdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIHQBcjcEy3+fc28AAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yO_zgsceL7Kv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}